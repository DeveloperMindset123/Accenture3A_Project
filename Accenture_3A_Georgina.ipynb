{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T19:20:31.132750700Z",
     "start_time": "2023-09-09T19:20:31.117340600Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import the neccessary dependacies we will use by default\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3a5b2fc17a89d0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Some insights into this project:\n",
    "- The category of machine learning models that these fall into is _supervised learning_. Supervised learning is a type of machine learning where the model is trained on labeled data. The label in this case is the method of data breach.\n",
    "\n",
    "The models that will be tested out will fall into the following category of machine learning:\n",
    "* <u><b>Logistic Regression</b></u>: This is a supervised Learning that can be used to predict a categorical outcome. In this case, the categorical outcome is the method of the data breach. The features that will be used are: entity, year, records and organization type.\n",
    "* <u><b>Decision Trees</b></u>: This is another supervised learning algorithm that can be used to predict a categorical outcome. Decision trees work by creating a tree-like structure that represents the relationships between the features and the outcomes.\n",
    "* <u><b>Support Vector Machine(SVMs):</u></b> This is a supervised learning algorithm that can be used to predict both categorical and continous outcomes. SVMs work by finding the hyperplane that best seperates the data points into different classes.\n",
    "* <u><b>Random Forests</u></b> This is an ensemble learning algorithm that combines multiple decision trees to improve the accuracy of the predictions.\n",
    "* <u><b>Neural Networks</u></b> This is a more complex algorithm that can be used to predict both categorical and continuous outcomes. Neural Networks work by learning the relationships between the features and the outcome through a process called backpropogation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4a0b8ccba56a7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T19:20:31.530128100Z",
     "start_time": "2023-09-09T19:20:31.452203700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load the dataset(s) we will be using\n",
    "csv_file_path = os.path.abspath('df_1.csv');  # get the absolute path of the CS\n",
    "\n",
    "df = pd.read_csv(csv_file_path);   # Read the CSV file into a datafram\n",
    "# display the head to see if the dataset works as intended\n",
    "df.head(10)  #adjust the parameter value as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a04c664",
   "metadata": {},
   "source": [
    "From my exploration, I discovered that dataframe 3 and dataframe 1 are the same, so merging them would be useless, dataframe 2 is different from dataframe 1, therefore, merging them holds some value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1621f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes  # simply lists out the datatype we are working with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b9e109abfaf428",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T19:20:33.325609Z",
     "start_time": "2023-09-09T19:20:33.265630700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.columns # observe the column list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cc115ab662e2dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T19:20:33.816103200Z",
     "start_time": "2023-09-09T19:20:33.785937900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we will need to rename the unnamed column\n",
    "df.rename(columns={'Unnamed: 0' : 'Index'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20893287fb448ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T19:20:35.163638500Z",
     "start_time": "2023-09-09T19:20:35.143525800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test out the number of unique values contained in Entity\n",
    "len(df['Entity'].unique())   #there's a total of 331 unique name for comapnies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e538d1fc6af68ec2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Regarding the column __Entity__, there's 2 options in terms of data preprocessing:\n",
    "* Drop the column altogether\n",
    "* Apply Hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ef5e43a5667284",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T19:20:37.408211600Z",
     "start_time": "2023-09-09T19:20:37.349619700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "#create a hash function\n",
    "def hash_function(text):\n",
    "    return hashlib.sha256(text.encode()).hexdigest()\n",
    "\n",
    "# create a new column for the hashed values\n",
    "df['hashed_Entity'] = df['Entity'].apply(hash_function)\n",
    "\n",
    "#observe the hashed column\n",
    "df.head()  # we have the hashed entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccce59fbb9cacbd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T19:20:37.594094400Z",
     "start_time": "2023-09-09T19:20:37.545021700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a copy of the dataframe and drop the Entity column \n",
    "df_copy1 = df.drop(columns={'Entity'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e84958bb201b54a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T19:20:37.803496900Z",
     "start_time": "2023-09-09T19:20:37.688758800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_copy1  # we don't want to make modifications to the original dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8603b602fd5debd5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T19:20:03.417380100Z",
     "start_time": "2023-09-09T19:20:02.813591Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(df_copy1['Organization type'].unique())   # in terms of organization type, there's 70 different unique values\n",
    "df_copy1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dc8a351025f36a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T19:20:03.417380100Z",
     "start_time": "2023-09-09T19:20:02.833091200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove the special characters\n",
    "df_copy1['Organization type'] = df_copy1['Organization type'].str.replace(',', '_')  # replace the cases of ',' with '_'.\n",
    "df_copy1['Organization type'] = df_copy1['Organization type'].str.replace(' ', '_') # we also replace the cases of spacing with '_'.\n",
    "df_copy1['Organization type'] = df_copy1['Organization type'].str.replace('__', '_')  # replace all instances of __ with _"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bdeb53",
   "metadata": {},
   "source": [
    "Before encoding the organization type, I've copied the df to keep the original data intact. I'm dropping the hashed_Entity column as it is not needed for visualizing – we want to focus on industries and not individual companies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ce8d0263117907",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T19:20:03.417380100Z",
     "start_time": "2023-09-09T19:20:02.847272400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_vis = df_copy1  #create another copy of the df_copy and perform further data preprocessing\n",
    "#drop the hashed entity column\n",
    "df_vis.drop(columns={'hashed_Entity'}, inplace=True)  #note: after dropping the column the first time, you may end up running into an error if you were to rerun the code block again\n",
    "#drop the Sources column\n",
    "df_vis.drop(columns={'Sources'}, inplace=True)\n",
    "df_vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to store index positions and non-integer values as tuples\n",
    "non_integer_records = []\n",
    "\n",
    "# Initialize a set to store unique industries\n",
    "unique_industries = set()\n",
    "\n",
    "# Iterate through the 'Records' column and collect non-integer values with their index positions and industry\n",
    "for index, (value, industry) in enumerate(zip(df_vis['Records'], df_vis['Organization type'])):\n",
    "    try:\n",
    "        int_value = int(value)\n",
    "    except ValueError:\n",
    "        non_integer_records.append((index, str(value), industry))\n",
    "        unique_industries.add(industry)\n",
    "\n",
    "# Print the index positions, values, and industries for non-integer values\n",
    "if non_integer_records:\n",
    "    print(\"Non-integer values in the 'Records' column:\")\n",
    "    print(\"Index, Value, Industry\")\n",
    "    for index, value, industry in non_integer_records:\n",
    "        print(index, value, industry)\n",
    "else:\n",
    "    print(\"No non-integer values in the 'Records' column.\")\n",
    "    \n",
    "# Print the list of unique industries from non-integer records\n",
    "if unique_industries:\n",
    "    print(\"Unique industries from non-integer records:\")\n",
    "    print(list(unique_industries))\n",
    "else:\n",
    "    print(\"No unique industries found in non-integer records.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_non_integer_records = []\n",
    "# types = list()\n",
    "\n",
    "for industry in unique_industries:\n",
    "    # Create a separate DataFrame for the current industry\n",
    "    industry_df = df_vis[df_vis['Organization type'] == industry]\n",
    "    \n",
    "    # Drop entries with non-integer values in 'Records'\n",
    "    industry_df['Records'] = pd.to_numeric(industry_df['Records'], errors='coerce')\n",
    "    industry_df = industry_df.dropna()\n",
    "    \n",
    "    # Calculate the mean records for the current industry\n",
    "    mean_records = industry_df['Records'].mean()\n",
    "    \n",
    "    # #add the type of mean_records to the types list\n",
    "    # types.append(type(mean_records))\n",
    "    \n",
    "    # Update 'Records' in the original DataFrame if mean is not NaN and remove from non_integer_records\n",
    "    #if the mean_record is nan, its type will be float\n",
    "    if type(mean_records) != float:\n",
    "        # Print the mean records for the current industry\n",
    "        # print(f\"Industry: {industry}, Type: {type(mean_records)}, Mean Records: {mean_records}\")\n",
    "        for index, value, industry_name in non_integer_records:\n",
    "            if industry_name == industry:\n",
    "                df_vis.at[index, 'Records'] = mean_records\n",
    "    else:\n",
    "        updated_non_integer_records.append(industry)\n",
    "\n",
    "#make the types list a set\n",
    "# types = set(types)\n",
    "# print(f\"Types of mean_records: {types}\")\n",
    "print(f\"Updated non-integer records: {updated_non_integer_records}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean records value for the entire DataFrame df_vis (excluding non-numeric values)\n",
    "df_vis['Records'] = pd.to_numeric(df_vis['Records'], errors='coerce')\n",
    "df_vis = df_vis.dropna()\n",
    "mean_records_all = df_vis['Records'].mean()\n",
    "\n",
    "# Replace 'Records' values in updated_non_integer_records with the mean records for the entire DataFrame\n",
    "for industry in updated_non_integer_records:\n",
    "    # for each entry in the dataframe, if the industry matches the industry in the updated_non_integer_records, replace the value with the mean_records_all\n",
    "    for index, value, industry_name in non_integer_records:\n",
    "        if industry_name == industry:\n",
    "            df_vis.at[index, 'Records'] = mean_records_all\n",
    "\n",
    "#Check that there are no more non-integer values in 'Records'\n",
    "non_integer_records = []\n",
    "for index, value, industry in zip(df_vis.index, df_vis['Records'], df_vis['Organization type']):\n",
    "    try:\n",
    "        int_value = int(value)\n",
    "    except ValueError:\n",
    "        non_integer_records.append((index, str(value), industry))\n",
    "\n",
    "if non_integer_records:\n",
    "    print(\"Non-integer values in the 'Records' column:\")\n",
    "    print(\"Index, Value, Industry\")\n",
    "    for index, value, industry in non_integer_records:\n",
    "        print(index, value, industry)\n",
    "else:\n",
    "    print(\"No remaining non-integer values in the 'Records' column!\")\n",
    "\n",
    "#Update all the values in the Records column to be integers\n",
    "df_vis['Records'] = df_vis['Records'].astype(int)\n",
    "\n",
    "# Check the data types of the DataFrame in the 'Records' column\n",
    "print(df_vis.dtypes)\n",
    "print(df_vis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac633714",
   "metadata": {},
   "source": [
    "Checking if the Year column is well formatted (should be a year like 2016, or 2019, not 2016-2019). If not, we'll need to do some data cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51564148",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if any value in the year column is null\n",
    "print(\"Null values: \", df_vis['Year'].isnull().values.any())  # there are no null values in the year column\n",
    "\n",
    "#Check if any value in the year column is not well formatted (i.e. not a number)\n",
    "print(\"Non numeric values: \", df_vis['Year'].str.isnumeric().values.any())  # there are values that are not numeric\n",
    "\n",
    "#print out all the values in the year column that are not numeric\n",
    "print(df_vis[~df_vis['Year'].str.isnumeric()])  # as we can see, the three columns that have non-numeric values are in index 94 96 and 144\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40deac98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the index column\n",
    "df_vis.drop(columns={'Index'}, inplace=True)\n",
    "#observe the dataset to see if the column index has been successfully dropped\n",
    "print(df_vis)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c521a8b",
   "metadata": {},
   "source": [
    "There are 3 values in the Year column that are not well formed – we also will need to fix the Records column for similar formatting issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a650533",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Modify df_vis:\n",
    "for each value in the year column that is not numeric:\n",
    "    record the last year listed in the year column (i.e. the last 4 characters)\n",
    "    record the first year listed in the year column (i.e. the first 4 characters)\n",
    "change the year column of this entry to the first 4 characters (the first year)\n",
    "for each year between the first year and the last year:\n",
    "    add a new entry to the dataframe with the same values as the entry that was changed, except for the year column, which will be the year in question\n",
    "'''\n",
    "last_row_index = df_vis.tail(1).index[0]\n",
    "# Create an empty list to store modified rows\n",
    "new_rows = []\n",
    "\n",
    "# Iterate through the DataFrame\n",
    "for index, row in df_vis.iterrows():\n",
    "    year_value = row['Year']\n",
    "    \n",
    "    # Check if the year is not numeric\n",
    "    if not year_value.isnumeric():\n",
    "        # Extract the first and last year\n",
    "        first_year = int(year_value[:4])\n",
    "        last_year = int(year_value[-4:])\n",
    "        \n",
    "        # Calculate the range of years and ensure it's at least 1\n",
    "        year_range = max(last_year - first_year, 0) + 1\n",
    "\n",
    "        # Change the year column to the first year\n",
    "        df_vis.loc[index, 'Year'] = str(first_year)\n",
    "        \n",
    "        # Calculate the records divided by the number of years in the range\n",
    "        records_divided = row['Records'] / year_range\n",
    "        \n",
    "        # Update the records column with the new value\n",
    "        df_vis.loc[index, 'Records'] = records_divided\n",
    "        \n",
    "        #Create new rows for each year between the first and last year\n",
    "        for year in range(first_year + 1, last_year + 1):\n",
    "            last_row_index+=1 # Increment the index of the last row\n",
    "            new_row = row.copy()  # Create a copy of the current row\n",
    "            new_row['Year'] = str(year)\n",
    "            # new_row['Index'] = last_row_index\n",
    "            new_row['Records'] = records_divided\n",
    "            new_rows.append(new_row) # Append the new row to the list\n",
    "\n",
    "# Concatenate the new rows with the original DataFrame\n",
    "new_rows_df = pd.DataFrame(new_rows)\n",
    "\n",
    "df_vis = pd.concat([df_vis, new_rows_df], ignore_index=True)\n",
    "\n",
    "# Convert Records to int\n",
    "df_vis['Records'] = df_vis['Records'].astype(int)\n",
    "\n",
    "# convert the year column to int\n",
    "df_vis['Year'] = df_vis['Year'].astype(int)\n",
    "\n",
    "\n",
    "print(df_vis.dtypes)\n",
    "print(df_vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa512522e893902e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T19:20:03.417380100Z",
     "start_time": "2023-09-09T19:20:02.878799600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#count the unique values in the organization column\n",
    "print(\"Organizations: \", len(df_vis['Organization type'].unique()))  # there are 60 values in the organization column\n",
    "\n",
    "#for each organization in the organization column, split the name on \"_\" and take the first word. Make a dictionary with the first word as the key and the number of times it appears as its valuee\n",
    "organization_dict = {}\n",
    "for organization in df_vis['Organization type'].unique():\n",
    "    first_word = organization.split('_')[0]\n",
    "    if first_word in organization_dict:\n",
    "        organization_dict[first_word] += 1\n",
    "    else:\n",
    "        organization_dict[first_word] = 1\n",
    "#for each key in the dictionary, if it appears more than once, print it out as well as the number of times it appears\n",
    "for key in organization_dict:\n",
    "    if organization_dict[key] > 1:\n",
    "        print(key, organization_dict[key])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should combine industries that are similar. For example, financial and financial services should be combined into one category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each organization in the dictionary, if it appears more than once, replace the organization column with the first word of the organization\n",
    "for index, row in df_vis.iterrows():\n",
    "    organization = row['Organization type']\n",
    "    first_word = organization.split('_')[0]\n",
    "    if organization_dict[first_word] > 1:\n",
    "        df_vis.loc[index, 'Organization type'] = first_word\n",
    "# check how many unique values there are in the organization column\n",
    "print(\"Unique Organizations: \", len(df_vis['Organization type'].unique()))\n",
    "\n",
    "print(df_vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if there are any null values in each column\n",
    "if df_vis.isnull().values.any():\n",
    "    print(\"There are null values in the dataframe\")\n",
    "else:\n",
    "    print(\"There are no null values in the dataframe\")\n",
    "\n",
    "print(df_vis.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the lowest and highest year\n",
    "print(\"Lowest Year: \", df_vis['Year'].min())\n",
    "print(\"Highest Year: \", df_vis['Year'].max())\n",
    "\n",
    "# print the lowest and highest records\n",
    "print(\"Lowest Records: \", df_vis['Records'].min())\n",
    "print(\"Highest Records: \", df_vis['Records'].max())\n",
    "print()\n",
    "\n",
    "#print the top 3 organizations with the highest records\n",
    "print(df_vis.groupby('Organization type')['Records'].sum().sort_values(ascending=False).head(3))\n",
    "print()\n",
    "\n",
    "#print the total records for the healthcare industry\n",
    "print(\"Total records lost in the healthcare industry: \")\n",
    "print(df_vis[df_vis['Organization type'] == 'healthcare']['Records'].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping the data by year\n",
    "grouped_data = df_vis.groupby('Year').size()\n",
    "\n",
    "# Sorting the grouped data by year\n",
    "grouped_data = grouped_data.sort_index()\n",
    "\n",
    "# Creating a bar plot with figsize\n",
    "fig, ax = plt.subplots(figsize=(25, 12))\n",
    "ax.bar(grouped_data.index, grouped_data.values)\n",
    "\n",
    "# Adding labels and title\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_ylabel('Number of Entries')\n",
    "ax.set_title('Number of Entries per Year')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data by year and sum the records for each year\n",
    "data_lost_over_time = df_vis.groupby('Year')['Records'].sum()\n",
    "\n",
    "# Create a line plot for data lost over time\n",
    "data_lost_over_time.plot(kind='line', figsize=(25, 12))\n",
    "plt.title(\"Data Lost Over Time\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Total Data Lost\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data by industry and sum the records for each industry, then select the top 20 industries\n",
    "top_20_industries = df_vis.groupby('Organization type')['Records'].sum().nlargest(20)\n",
    "\n",
    "# Create a bar plot for the top 10 industries with the highest data loss\n",
    "top_20_industries.plot(kind='bar', figsize=(25, 12))\n",
    "plt.title(\"Top 20 Industries with Highest Data Loss\")\n",
    "plt.xlabel(\"Industry\")\n",
    "plt.ylabel(\"Total Data Lost\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data by the method and count the occurrences, then select the top 10 causes\n",
    "top_10_causes = df_vis['Method'].value_counts().nlargest(10)\n",
    "\n",
    "# Create a bar plot for the top 10 causes of data breaches\n",
    "top_10_causes.plot(kind='bar', figsize=(25, 12))\n",
    "plt.title(\"Top 10 Causes of Data Breaches\")\n",
    "plt.xlabel(\"Cause\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping the data by year and industry and summing the 'Records' column\n",
    "records_by_year_and_industry = df_vis.groupby(['Year', 'Organization type'])['Records'].sum().unstack()\n",
    "\n",
    "# Finding the top 20 industries based on total records\n",
    "top_20_industries = records_by_year_and_industry.sum().sort_values(ascending=False).head(20).index\n",
    "\n",
    "# Filtering the DataFrame to include only the top 20 industries\n",
    "records_by_year_and_industry_top20 = records_by_year_and_industry[top_20_industries]\n",
    "\n",
    "# Creating a stacked bar plot for records by year and industry (top 20)\n",
    "records_by_year_and_industry_top20.plot(kind='bar', stacked=True, figsize=(25, 12))\n",
    "\n",
    "# Adding labels and title\n",
    "plt.title(\"Stacked Bar Plot of Records by Year and Top 20 Industries\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Total Records\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend(title='Industry Type', bbox_to_anchor=(1.05, 1), loc='upper left')  # Adjust legend position\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping the data by year and industry and summing the 'Records' column\n",
    "records_by_year_and_industry = df_vis.groupby(['Year', 'Organization type'])['Records'].sum().unstack()\n",
    "\n",
    "# Finding the top 20 industries based on total records\n",
    "top_20_industries = records_by_year_and_industry.sum().sort_values(ascending=False).head(13).index\n",
    "\n",
    "# Filtering the DataFrame to include all industries and filling NaN values with 0\n",
    "records_by_year_and_industry_all = records_by_year_and_industry.fillna(0)\n",
    "\n",
    "# Create a consolidated image for all pie charts\n",
    "plt.figure(figsize=(15, 15))\n",
    "\n",
    "# Create subplots based on the number of years\n",
    "num_years = len(records_by_year_and_industry_all.index)\n",
    "num_cols = 3  # Adjust the number of columns based on your preference\n",
    "num_rows = (num_years + num_cols - 1) // num_cols\n",
    "\n",
    "for i, year in enumerate(records_by_year_and_industry_all.index, start=1):\n",
    "    data_for_year = records_by_year_and_industry_all.loc[year]\n",
    "\n",
    "    # Calculate the percentage of records lost in the healthcare industry and the rest\n",
    "    healthcare_percentage = data_for_year['healthcare'] / data_for_year.sum()\n",
    "    other_percentage = 1 - healthcare_percentage\n",
    "\n",
    "    # Create subplots\n",
    "    plt.subplot(num_rows, num_cols, i)\n",
    "    plt.pie([healthcare_percentage, other_percentage], labels=['healthcare', 'other'],\n",
    "            autopct=lambda p: '{:.1f}%'.format(p) if p > 0 else '',\n",
    "            startangle=90)\n",
    "    plt.title(f\"{year}\")\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform one-hot encoding on 'Organization type' and 'Method'\n",
    "df_encoded = pd.get_dummies(df_vis, columns=['Organization type', 'Method'], prefix=['Org', 'Method'])\n",
    "\n",
    "# Display the one-hot encoded DataFrame\n",
    "print(df_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression to predict Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_encoded.drop(['Records'], axis=1)  # Features (excluding the target variable)\n",
    "y = df_encoded['Records']  # Target variable\n",
    "\n",
    "# Create a linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Fit the model on the entire data\n",
    "model.fit(X, y)\n",
    "\n",
    "# Make predictions on the features\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# Add the predicted values to the DataFrame\n",
    "df_encoded['Predicted_Records'] = y_pred\n",
    "\n",
    "# Create a regression plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.regplot(x='Records', y='Predicted_Records', data=df_encoded, scatter_kws={'s': 10}, line_kws={'color': 'red'})\n",
    "plt.title('Regression Plot: Actual vs Predicted Records')\n",
    "plt.xlabel('Actual Records')\n",
    "plt.ylabel('Predicted Records')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, that's not a great performance. Let's try to handle outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.mstats import winsorize\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "df_encoded1 = df_vis.copy()\n",
    "\n",
    "# Extract features (X) and target variable (y)\n",
    "X = df_encoded1[['Organization type']]\n",
    "y = df_encoded1['Records']\n",
    "\n",
    "# One-hot encode the categorical variable 'Organization type'\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('org_type', OneHotEncoder(), ['Organization type'])\n",
    "    ])\n",
    "\n",
    "# Create a linear regression model\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "# Fit the model on the data\n",
    "model.fit(X, y)\n",
    "\n",
    "# Make predictions on the features\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# Add the predicted values to the DataFrame\n",
    "df_encoded1['Predicted_Records'] = y_pred\n",
    "\n",
    "# Create a regression plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.regplot(x='Records', y='Predicted_Records', data=df_encoded1, scatter_kws={'s': 10}, line_kws={'color': 'red'})\n",
    "plt.title('Regression Plot: Actual vs Predicted Records')\n",
    "plt.xlabel('Actual Records')\n",
    "plt.ylabel('Predicted Records')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not much better. Let's look at some clustering while we reconsider our approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_cluster = df_vis.copy()\n",
    "\n",
    "# Select features for clustering\n",
    "features = df_cluster[['Year', 'Records']]\n",
    "\n",
    "# Standardize the features (important for K-means)\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# Choose the number of clusters \n",
    "num_clusters = 2\n",
    "\n",
    "# Apply K-means clustering\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "df_cluster['Cluster'] = kmeans.fit_predict(features_scaled)\n",
    "\n",
    "# Visualize the clusters\n",
    "plt.scatter(df_cluster['Year'], df_cluster['Records'], c=df_cluster['Cluster'], cmap='viridis')\n",
    "plt.title('K-means Clustering')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Records')\n",
    "plt.show()\n",
    "\n",
    "# Create cross-tabulation for 'Organization type'\n",
    "organization_type_cross_tab = pd.crosstab(df_cluster['Cluster'], df_cluster['Organization type'], margins=True, margins_name='Total')\n",
    "\n",
    "# Create cross-tabulation for 'Method of hacking'\n",
    "method_of_hacking_cross_tab = pd.crosstab(df_cluster['Cluster'], df_cluster['Method'], margins=True, margins_name='Total')\n",
    "\n",
    "# Plot heatmaps\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.heatmap(organization_type_cross_tab, annot=True, cmap='coolwarm', fmt='d')\n",
    "plt.title('Organization Type Cross-Tabulation')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.heatmap(method_of_hacking_cross_tab, annot=True, cmap='coolwarm', fmt='d')\n",
    "plt.title('Method of Hacking Cross-Tabulation')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8136bba27fff91b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T19:20:03.425964400Z",
     "start_time": "2023-09-09T19:20:02.896640200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder()  #perform label encoding on the organization_type, the copy of df_vis\n",
    "\n",
    "# implement label encoding on the Organization type column\n",
    "le.fit(df_vis['Organization type'])   # fit the data we want to train the encoder on\n",
    "df_vis['Organization type'] = le.transform(df_vis['Organization type'])\n",
    "# observe how the column 'Organization Type' has changed\n",
    "df_vis.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beeae597c9415ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T19:20:03.427983800Z",
     "start_time": "2023-09-09T19:20:02.925385200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_vis.dtypes   # as we can see, the Organization type changed from Object --> integer datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9bc04877c52067",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T19:20:03.427983800Z",
     "start_time": "2023-09-09T19:20:02.940609300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# observe the label frequency, to gain an understanding of outliers and inlier values\n",
    "df_vis['Organization type'].value_counts()    # some values to note: 7 repeats 13 times, 18 repeats 38  times, 23 repeats 12 times, 25 repeats 30 times, 30 repeats 47 times, 49 repeats 27 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2f14c41cfaefbd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T19:20:03.427983800Z",
     "start_time": "2023-09-09T19:20:02.955234800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_vis['Method']  # we will also need to perform label encoding on the method section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d34f54c0a578de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T19:20:03.427983800Z",
     "start_time": "2023-09-09T19:20:02.974881300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_vis['Method'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c301eddc412f9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T19:20:03.432040600Z",
     "start_time": "2023-09-09T19:20:02.991663500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"# seems like the method column contains certain NaN values and certain Unknown values, we will need to determine whether we replace such values or drop them in its entirety\n",
    "df_vis['Method'].value_counts()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe34e170",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check which columns have null/unknoown values\n",
    "null_mask = df.isnull().any(axis=1)\n",
    "null_rows=df[null_mask]\n",
    "\n",
    "print(null_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696b076f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop sources and index\n",
    "#df_vis.drop(columns=['Index', 'Sources'], axis=1, inplace=True)\n",
    "df_vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bd475012cbaf9a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T19:20:03.432040600Z",
     "start_time": "2023-09-09T19:20:03.002592600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Two changes needs to be made regarding the method column\n",
    "    1. Replace the \"Unknown\" value with the most frequently repeated word\n",
    "    2. Drop any NaN values if it exists\n",
    "\"\"\"\n",
    "most_frequent_word = df_vis['Method'].value_counts().index[0]\n",
    "#replace all occurences of \"Unknown\" with \"Hacked\"\n",
    "df_vis['Method'].replace('unknown', most_frequent_word, inplace=True)\n",
    "# drop any NaN values\n",
    "df_vis.dropna(inplace=True)\n",
    "# Print the dataframe\n",
    "df_vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637d2b426b89eef3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T19:20:03.432040600Z",
     "start_time": "2023-09-09T19:20:03.034179100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let's verify if the method column contains any nan/null values\n",
    "df_vis.isnull().all()   # seems that none of the values here contain any more null values, as we can see none of the datatype has any null values now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec8a20de5f47bd4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T19:20:03.438779200Z",
     "start_time": "2023-09-09T19:20:03.049184600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(list(df_vis['Method'].unique()))   # based on the list, we can see that the Method column doesn't contain unknown anymore.\n",
    "list(df_vis['Method'].value_counts())   # we also gain insight into the frequency of the methods being repeated, there's 24 methods, therefore, the label encoding will range from 0-23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc22b2bec14c0fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T19:20:03.740022400Z",
     "start_time": "2023-09-09T19:20:03.066149600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# prior to implementing label encoding, we will need to clear up the string of some grammatical issues that will cause mismatched data when we train the model otherwise\n",
    "# we want uniformity for the data input values\n",
    "# convert all uppercase letters to lowercase\n",
    "df_vis['Method'] = df_vis['Method'].str.lower()\n",
    "\n",
    "# replace all spaces with \"_\"\n",
    "df_vis['Method'] = df_vis['Method'].str.replace(' ', '_')\n",
    "\n",
    "# replace all slash signs with \"_\"\n",
    "df_vis['Method'] = df_vis['Method'].str.replace('/', '_')\n",
    "\n",
    "# replace all __ and ___ with _\n",
    "df_vis['Method'] = df_vis['Method'].str.replace('___', '_')\n",
    "df_vis['Method'] = df_vis['Method'].str.replace('__', '_')\n",
    "\n",
    "# check the updated dataframe\n",
    "list(df_vis['Method'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226c00a1c46c21c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T19:20:03.740022400Z",
     "start_time": "2023-09-09T19:20:03.112754400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now the method column is ready for label encoding preprocessing, since we already called on the label encoder previously, we can reuse it\n",
    "le.fit(df_vis['Method'])  # train the label encoder on the column data we want to train\n",
    "df_vis['Method'] = le.transform(df_vis['Method'])\n",
    "df_vis   # observe that method has been successfully encoded by label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb398959",
   "metadata": {},
   "outputs": [],
   "source": [
    "#continue here  --> https://machinelearningmastery.com/building-a-regression-model-in-pytorch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cc7ca0e870548e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T19:20:03.740022400Z",
     "start_time": "2023-09-09T19:20:03.149368100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_copy1['Method'].value_counts()  # judging by the values shown here, seems like 4 represents 'hacked'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012aba13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the inverse_transform method if you need to decode the method back to the original text\n",
    "original_text = le.inverse_transform([4])\n",
    "original_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538534f6dc02a826",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T19:20:03.740022400Z",
     "start_time": "2023-09-09T19:20:03.176187100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_copy1.dtypes  # reobserve the data, as we can see, the columns that are of object datatype needs to be changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a500f423ffe8d0bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T19:20:03.740022400Z",
     "start_time": "2023-09-09T19:20:03.232405900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "object_to_numeric = ['Year', 'Records', 'hashed_Entity']\n",
    "df_copy1[object_to_numeric] = df_copy1[object_to_numeric].apply(pd.to_numeric, errors=\"ignore\", axis=1)   # we have successfully converted the dataframe from object to float, this ensures it's ready to be trained using machine learning model\n",
    "df_copy1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ec8854588b7b18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T19:20:03.740022400Z",
     "start_time": "2023-09-09T19:20:03.286655600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_copy1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb87cac2c1e7490",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T19:20:03.740022400Z",
     "start_time": "2023-09-09T19:20:03.317731Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_copy1.isnull().values.any()   # the entirety of the dataframe does not contain any null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862344dad3f7099",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T19:20:03.748110100Z",
     "start_time": "2023-09-09T19:20:03.335555Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943417e8484d5b66",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Marks the end of the data preprcoessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40821708fe9172b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T19:20:03.844269800Z",
     "start_time": "2023-09-09T19:20:03.395959700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# using the original tabel where the name of the Entities as well as the hashed entities are together, using that we can create a lookup table in the form of a dictionary\n",
    "\n",
    "dictionary = {}  # create a dictionary that maps the hashed company names to the original company names\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    hashed_company_name = row[\"hashed_Entity\"]\n",
    "    original_company_name = row[\"Entity\"]\n",
    "    dictionary[hashed_company_name] = original_company_name\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04373ced2ac0e6c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T19:20:03.944673Z",
     "start_time": "2023-09-09T19:20:03.417380100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dictionary   # we have successfully created a dictionary that maps the hashed values to the name of the original companues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19035cda3ece7ea9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T19:20:03.956703Z",
     "start_time": "2023-09-09T19:20:03.448668500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add a new column to the DataFrame that maps the hashed company names to the original company names\n",
    "df_copy1[\"original_Entity\"] = df[\"hashed_Entity\"].apply(lambda x: dictionary[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8172449e30301788",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T19:20:04.001324100Z",
     "start_time": "2023-09-09T19:20:03.466481400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_copy1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
